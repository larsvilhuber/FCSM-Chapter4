
@article{barrientos_providing_2018,
	title = {Providing access to confidential research data through synthesis and verification: {An} application to data on employees of the {U}.{S}. federal government},
	volume = {12},
	issn = {1932-6157},
	shorttitle = {Providing access to confidential research data through synthesis and verification},
	url = {https://projecteuclid.org/euclid.aoas/1532743488},
	doi = {10.1214/18-AOAS1194},
	language = {en},
	number = {2},
	urldate = {2020-03-12},
	journal = {The Annals of Applied Statistics},
	author = {Barrientos, Andrés F. and Bolton, Alexander and Balmat, Tom and Reiter, Jerome P. and de Figueiredo, John M. and Machanavajjhala, Ashwin and Chen, Yan and Kneifel, Charley and DeLong, Mark},
	month = jun,
	year = {2018},
	pages = {1124--1156}
}

@article{barrientos_providing_2018-1,
	title = {Providing {Access} to {Confidential} {Research} {Data} {Through} {Synthesis} and {Verification}: {An} {Application} to {Data} on {Employees} of the {U}.{S}. {Federal} {Government}},
	shorttitle = {Providing {Access} to {Confidential} {Research} {Data} {Through} {Synthesis} and {Verification}},
	url = {http://arxiv.org/abs/1705.07872},
	abstract = {Data stewards seeking to provide access to large-scale social science data face a difficult challenge. They have to share data in ways that protect privacy and confidentiality, are informative for many analyses and purposes, and are relatively straightforward to use by data analysts. One approach suggested in the literature is that data stewards generate and release synthetic data, i.e., data simulated from statistical models, while also providing users access to a verification server that allows them to assess the quality of inferences from the synthetic data. We present an application of the synthetic data plus verification server approach to longitudinal data on employees of the U.S. federal government. As part of the application, we present a novel model for generating synthetic career trajectories, as well as strategies for generating high dimensional, longitudinal synthetic datasets. We also present novel verification algorithms for regression coefficients that satisfy differential privacy. We illustrate the integrated use of synthetic data plus verification via analysis of differentials in pay by race. The integrated system performs as intended, allowing users to explore the synthetic data for potential pay differentials and learn through verifications which findings in the synthetic data hold up and which do not. The analysis on the confidential data reveals pay differentials across races not documented in published studies.},
	urldate = {2020-03-12},
	journal = {arXiv:1705.07872 [stat]},
	author = {Barrientos, Andrés F. and Bolton, Alexander and Balmat, Tom and Reiter, Jerome P. and de Figueiredo, John M. and Machanavajjhala, Ashwin and Chen, Yan and Kneifel, Charley and DeLong, Mark},
	month = jun,
	year = {2018},
	note = {arXiv: 1705.07872},
	keywords = {Statistics - Applications}
}

@article{reiter_verification_2009,
	title = {Verification servers: {Enabling} analysts to assess the quality of inferences from public use data},
	volume = {53},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947308004751},
	doi = {10.1016/j.csda.2008.10.006},
	abstract = {To protect confidentiality, statistical agencies typically alter data before releasing them to the public. Ideally, although generally not done, the agency also provides a way for secondary data analysts to assess the quality of inferences obtained with the released data. Quality measures can help secondary data analysts to identify inaccurate conclusions resulting from the disclosure limitation procedures, as well as have confidence in accurate conclusions. We propose a framework for an interactive, web-based system that analysts can query for measures of inferential quality. As we illustrate, agencies seeking to build such systems must consider the additional disclosure risks from releasing quality measures. We suggest some avenues of research on limiting these risks.},
	number = {4},
	journal = {Computational statistics \& data analysis},
	author = {Reiter, Jerome P and Oganian, Anna and Karr, Alan F},
	month = feb,
	year = {2009},
	pages = {1475--1482}
}

@article{abowd_economic_2015,
	title = {Economic {Analysis} and {Statistical} {Disclosure} {Limitation}},
	volume = {50},
	issn = {0007-2303},
	url = {http://www.brookings.edu/about/projects/bpea/papers/2015/economic-analysis-statistical-disclosure-limitation},
	doi = {10.1353/eca.2016.0004},
	abstract = {This paper explores the consequences for economic research of methods used by data publishers to protect the privacy of their respondents. We review the concept of statistical disclosure limitation for an audience of economists who may be unfamiliar with these methods. We characterize what it means for statistical disclosure limitation to be ignorable. When it is not ignorable, we consider the effects of statistical disclosure limitation for a variety of research designs common in applied economic research. Because statistical agencies do not always report the methods they use to protect confidentiality, we also characterize settings in which statistical disclosure limitation methods are discoverable; that is, they can be learned from the released data. We conclude with advice for researchers, journal editors, and statistical agencies.},
	number = {1},
	journal = {Brookings Papers on Economic Activity},
	author = {Abowd, John M and Schmutte, Ian M},
	year = {2015},
	keywords = {SDL, Sloan-annual-report, Statistical Disclosure Limitation, primary},
	pages = {221--267}
}

@article{kinney_towards_2011,
	title = {Towards {Unrestricted} {Public} {Use} {Business} {Microdata}: {The} {Synthetic} {Longitudinal} {Business} {Database}},
	volume = {79},
	issn = {0306-7734, 1751-5823},
	url = {http://dx.doi.org/10.1111/j.1751-5823.2011.00153.x},
	doi = {10.1111/j.1751-5823.2011.00153.x},
	abstract = {Dans la plupart des pays, les instituts nationaux de statistique ne publient pas les micro-données relatives aux entreprises. Les publier présente en effet un risque trop élevé de rupture de confidentialité. Ce risque peut être évité par un recours à des données synthétiques—des données simulées à partir de modèles statistiques reproduisant la loi des véritables micro-données. Dans cet article, nous décrivons une application de cette stratégie à la création d'une telle base de données à partir des résultats du recensement économique annuel des entreprises américaines. Cette base de donnée comprend plus de 20 millions d'entreprises sur une période remontant à 1976. L'U.S. Bureau of Census et l'Internal Revenue Service ont récemment approuvé la publication sous forme synthétique de ces micro-données, faisant ainsi de la Longitudinal Business Database le premier ensemble de micro-données de ce type accessible au public aux Etats-Unis. Nous expliquons la façon dont cette base de données synthétiques a été créée, comment sa validité a été testée, et comment son risque de rupture de confidentialité a étéévalué.},
	number = {3},
	journal = {International statistical review},
	author = {Kinney, Satkartar K and Reiter, Jerome P and Reznek, Arnold P and Miranda, Javier and Jarmin, Ron S and Abowd, John M},
	month = dec,
	year = {2011},
	keywords = {Economic census, Linked admin, data confidentiality, disclosure limitation, synthetic data},
	pages = {362--384}
}

@article{reiter_significance_2005,
	title = {Significance tests for multi-component estimands from multiply imputed, synthetic microdata},
	volume = {131},
	issn = {0378-3758},
	url = {http://www.sciencedirect.com/science/article/pii/S0378375804000771},
	doi = {10.1016/j.jspi.2004.02.003},
	abstract = {Abstract To limit the risks of disclosures when releasing data to the public, it has been suggested that statistical agencies release multiply imputed, synthetic microdata. For example, the released microdata can be fully synthetic, comprising random samples of units from the sampling frame with simulated values of variables. Or, the released microdata can be partially synthetic, comprising the units originally surveyed with some collected values, e.g. sensitive values at high risk of disclosure or values of key identifiers, replaced with multiple imputations. This article presents inferential methods for synthetic data for multi-component estimands, in particular procedures for Wald and likelihood ratio tests. The performance of the procedures is illustrated with simulation studies.},
	number = {2},
	journal = {Journal of statistical planning and inference},
	author = {Reiter, J P},
	month = may,
	year = {2005},
	keywords = {Disclosure, Multiple imputation, Significance tests, confidentiality, synthetic data},
	pages = {365--377}
}

@book{us_census_bureau_sipp_2015,
	address = {Washington, DC and Ithaca, NY, USA},
	title = {{SIPP} {Synthetic} {Beta} {Version} 6.0.2},
	url = {http://www2.vrdc.cornell.edu/news/data/sipp-synthetic-beta-file/},
	author = {{U.S. Census Bureau}},
	year = {2015}
}

@article{miranda_using_2016,
	title = {Using partially synthetic microdata to protect sensitive cells in business statistics},
	volume = {32},
	url = {https://content.iospress.com/articles/statistical-journal-of-the-iaos/sji963},
	doi = {10.3233/SJI-160963},
	abstract = {We describe and analyze a method that blends records from both observed and synthetic microdata into public-use tabulations on establishment statistics. The resulting tables use synthetic data only in potentially sensitive cells. We describe different algorithms, and present preliminary results when applied to the Census Bureau's Business Dynamics Statistics and Synthetic Longitudinal Business Database, highlighting accuracy and protection afforded by the method when compared to existing public-use tabulations (with suppressions).},
	number = {1},
	journal = {Statistical Journal of the International Association for Official Statistics},
	author = {Miranda, Javier and Vilhuber, Lars},
	year = {2016},
	keywords = {SDL, statistical disclosure limitation},
	pages = {69--80}
}

@article{drechsler_new_2012,
	title = {New data dissemination approaches in old {Europe} – synthetic datasets for a {German} establishment survey},
	volume = {39},
	issn = {0266-4763, 1360-0532},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02664763.2011.584523},
	doi = {10.1080/02664763.2011.584523},
	language = {en},
	number = {2},
	urldate = {2018-07-22},
	journal = {Journal of Applied Statistics},
	author = {Drechsler, Jörg},
	month = feb,
	year = {2012},
	pages = {243--265}
}

@misc{vilhuber_usage_2016,
	type = {Presentation},
	title = {Usage and outcomes of the {Synthetic} {Data} {Server}},
	url = {http://hdl.handle.net/1813/43883},
	language = {en},
	urldate = {2018-07-22},
	author = {Vilhuber, Lars and Abowd, John},
	year = {2016}
}

@article{nowok_synthpop_2016,
	title = {synthpop: {Bespoke} {Creation} of {Synthetic} {Data} in {R}},
	volume = {74},
	issn = {1548-7660},
	url = {https://www.jstatsoft.org/v074/i11},
	doi = {10.18637/jss.v074.i11},
	abstract = {In many contexts, confidentiality constraints severely restrict access to unique and valuable microdata. Synthetic data which mimic the original observed data and preserve the relationships between variables but do not contain any disclosive records are one possible solution to this problem. The synthpop package for R, introduced in this paper, provides routines to generate synthetic versions of original data sets. We describe the methodology and its consequences for the data characteristics. We illustrate the package features using a survey data example.},
	number = {11},
	journal = {Journal of Statistical Software, Articles},
	author = {Nowok, Beata and Raab, Gillian and Dibben, Chris},
	year = {2016},
	keywords = {CART, R, UK longitudinal studies, disclosure control, synthetic data},
	pages = {1--26}
}

@techreport{vilhuber_proceedings_2017,
	type = {Document},
	title = {Proceedings from the {Synthetic} {LBD} {International} {Seminar}},
	url = {http://digitalcommons.ilr.cornell.edu/ldi/44/},
	abstract = {On May 9, 2017, we hosted a seminar to discuss the conditions necessary to implement the SynLBD approach with interested parties, with the goal of providing a straightforward toolkit to implement the same procedure on other data. The proceedings summarize the discussions during the workshop.},
	number = {44},
	institution = {Labor Dynamics Institute, Cornell University},
	author = {Vilhuber, Lars and Kinney, Saki and Schmutte, Ian},
	year = {2017}
}

@article{reiter_verification_2009-1,
	title = {Verification servers: {Enabling} analysts to assess the quality of inferences from public use data},
	volume = {53},
	issn = {01679473},
	shorttitle = {Verification servers},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947308004751},
	doi = {10.1016/j.csda.2008.10.006},
	language = {en},
	number = {4},
	urldate = {2020-03-12},
	journal = {Computational Statistics \& Data Analysis},
	author = {Reiter, Jerome P. and Oganian, Anna and Karr, Alan F.},
	month = feb,
	year = {2009},
	pages = {1475--1482}
}