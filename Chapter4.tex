\documentclass[12pt]{article}

%%% All setup/preamble moved to a different file
\input{preamble.tex}

%%% IF YOU HAVE ANY QUESTIONS, LOOK AT THE README.MD

\begin{document}

\title{WP22 Chapter 4: Impact of restricted access models and SDL methods on data quality and usability}

\author{% 
Anna Oganian\thanks{National Center for Health Statistics}, 
Ellen Galantucci\thanks{Bureau of Labor Statistics}, \\
Donna Miller\thanks{National Center for Health Statistics}, 
Lars Vilhuber\thanks{U.S. Census Bureau and Cornell University}, \\
Simson Garfinkel\thanks{U.S. Census Bureau}
}

\maketitle
\newpage 
\begin{center}
    This part will be removed in the final version?
\end{center}
\tableofcontents

\newpage 

\begin{abstract}
When releasing data to the public, statistical agencies and survey
organizations typically alter data values in order to protect the
confidentiality of survey respondents' identities and attribute
values.  To select among the wide variety of data alteration
methods, agencies require tools for evaluating the utility of
proposed data releases.  Such utility measures can be combined
with disclosure risk measures to gauge risk-utility tradeoffs of
competing methods.  Some examples of utility metrics are presented in this Chapter.
A  decision-theoretic formulation for evaluating
disclosure limitation procedures based on utility and risk metrics is outlined
as well. Finally, examples of utility assessments of certain families of SDL methods are given. 


\end{abstract}

\section{Introduction}\label{sec.intro}

As it was described in Chapter 3, there is a wide range of \SDL\ techniques.
These \SDL\ methods can be implemented with differing degrees of
intensity.  Generally, increasing the amount of alteration
decreases the risk of disclosure, but it also decreases the
accuracy of inferences obtainable from the released data, often
referred to as data utility \citep{Hund10}.

So,  \SDL\ practitioner needs to decide which technique, and with what
degree of intensity to use in a particular setting of data release. 
In general the approach to this problem is to employ risk-utility formulations. We assume
below that each candidate release $R$ can be characterized by a quantified
\textit{disclosure risk} $\DR (R)$ and \textit{data utility} $\DU
(R)$.  Examples of $\DU(R)$ metrics are given in Section \ref{du_metrics}.
The particular released data, $DB_{REL}$, can be selected from
the candidates in one of two ways. The first is to maximize
utility subject to an upper bound on risk, by solving an
optimization problem of the form
\begin{equation}\label{eq.opt}
\begin{array}{l}
DB_{REL} = {\arg\max}_{R \in \RS} \DU (R) \\[1ex]
\mbox{s.t. } \DR(R) \leq \alpha
\end{array}
\end{equation}
where \RS\ is the set of all possible releases. 

The second and more flexible approach is to define \textit{risk-utility
frontiers} using the partial order $\preceq_{\mathrm{RU}}$ defined
by
\begin{equation}\label{eq.rupo}
R_1 \preceq_{\mathrm{RU}} R_2 \Leftrightarrow \DR(R_2) \leq
\DR(R_1) \qquad\mbox{and}\qquad \DU(R_2) \geq \DU(R_1).
\end{equation}

When $R_1 \preceq_{\mathrm{RU}} R_2$, the $R_2$ is preferred to
$R_1$ because it has both lower disclosure risk and higher
utility. Only candidate releases on the risk-utility frontier of
maximal elements of \RS\ with respect to the partial order
(\ref{eq.rupo}) need be considered further: for any other
candidate, some element of the frontier has lower risk
\textit{and} higher utility. Calculation of the frontier can be
done using existing algorithms for finding the maxima in a set of
vectors \citep{kung-luccio-preparata75}. 


The choice among the SDL methods lying on the risk-utility
frontier lies with the data disseminator. To illustrate the first approach
described above, consider Figure \ref{fig.ruplot}, where each point
represents some SDL method characterized in terms of Utility and Risk 
measured according to certain metrics on the horizontal and vertical axes respectively.
If the risk threshold were $10\%$ (in some settings, not a very conservative value), 
then a method denoted as \NOI\ would be the preferred SDL. It is also clear from Figure \ref{fig.ruplot} that compared to \MICZ\ or \NOI\, \MICI\ produces only a minor increase
in utility at an enormous cost in terms of disclosure risk. Similarly,
\RANK\ yields only a modest improvement in disclosure
risk over \MICP\ and \NOI\, but incurs an immense
penalty in terms of data utility. Thus, in a scenario represented by Figure \ref{fig.ruplot}
 the disseminator  might prefer \NOI\ or \MICP\ .

\begin{figure}
\begin{center}
\includegraphics[width=4in]{R_U_plot.pdf}
\end{center}
\caption{Risk-Utility plot}
\label{fig.ruplot}
\end{figure}


\section{Scope of this chapter}\label{sec:scope}

We reference previous chapters' coverage of \gls{SDL} and other methods that restrict access to the full confidential data, describe various metrics that might be used to assess the utility of such methods.

\section{Access Methods: A brief listings}\label{sec:access_methods}

Access methods are one component of a disclosure avoidance system.

\section{Disclosure Avoidance Methods}\label{sec:da_methods}

To structure this chapter, we outline our framework. 

When evaluating utility, we consider the final public output product - after all output controls. Figure~\ref{fig:framework} illustrates the data flow from confidential data to output product, going through "mechanisms" that are combinations of "access control" methods ("Safe settings", "Safe people", "Safe projects", and "safe outputs") and "SDL methods" ("safe data"). (terminology from the five safes framework.

\begin{figure}
    \centering
\includegraphics[width=0.8\textwidth]{SDL+Access control.png}
    \caption{Evaluation Framework}
    \label{fig:framework}
\end{figure}

\section{Non-Data Utility Metrics}\label{sec:other_metrics}
\begin{itemize}
    \item Ease of access
    \item Timeliness of outputs
    \item others?
\end{itemize}

\section{Data Utility metrics} \label{du_metrics}
In this section we present some examples of Data Utility metrics \DU\ that can be used by the data protector to assess the quality of the masked data, which can help to choose an appropriate approach for disclosure limitation.

In the broadest sense, the utility of a particular data release is the
benefit to society of the released information. Benefits this general
are nearly impossible to quantify and measure, because they depend on
more than simply the released data. A narrower, more feasible approach
is to characterize the quality of what can be learned from the masked
data relative to what can be learned from the original data. Such
comparisons can be tailored to specific analyses or can be broadened
to global differences in distributions. Examples of both approaches are presented in the corresponding sections below.

\subsection{Generic metrics of Data Utility}

Generic metrics capture global differences between the distributions of the original and masked data. 
One class of examples of generic measures are functions of the differences between point estimates of the first and second moments (and possibly other summaries) based on the original and masked data. Another is statistical distances between the distributions of the original and masked data \cite{dfks02, gks06}, for example Kullback-Leibler divergence.  When the data are approximately multivariate normal, the \KL\ captures the differences in the distributions of the entire data,
which in turn account for differences in inferences. 
However, the \KL\ measure is not easily
interpreted when the data, or some transformed version of the
data, are not reasonably well-described by a multivariate normal
distribution, that is why it's  usage as  global metric of data utility
would be limited.



\subsubsection{Propensity score utility metric}
% AO 03/31: start editing from here next time

An example of global utility metric that can be computed for the variables of different types is a propensity score based measure(\cite{propen}).

For any binary variable $T$, the propensity
score is defined as the probability that $T=1$ given covariate values $\bx$. \cite{rr83} show
that $T$ and $\bx$ are conditionally independent given the propensity
score. Thus, when two large groups have the same distributions of
propensity scores, the groups should have similar distributions of $\bx$.

This theory suggests an approach for measuring data utility.
First, we merge (by ``stacking'') the original and masked data sets, adding a variable $T$ that equals one for all records from the masked data set and equals zero for all records from the original data set. If
variables have been dropped as part of the masking, they are also
dropped in computation of propensity scores.  Second, for each record
in the original and masked data, we compute the probability of being
in the \textit{masked} data set---the propensity
score. Third, we compare the distributions of the propensity scores in
the original and masked data.  When those distributions are similar,
the distributions of the original and masked data are similar, and so
data utility should be relatively high.

Propensity scores can be estimated via a logistic regression of the
``masked/original'' variable $T$ on functions of all variables $\bx$
in the data set.  

The similarity of the propensity scores for the masked and original
observations can be assessed in numerous ways, for example comparisons
of their percentiles in each group.  A simple summary was proposed in \cite{propen}:
\begin{equation}
U_p =  \frac{1}{N} \sum_{i=1}^{N}\left[\hat{p}_i-c\right]^2,
\end{equation}
where $N$ is the total number of records in the merged data set,
$\hat{p}_i$ is the estimated propensity score for unit $i$, and $c$
equals the proportion of units with masked data in the merged data
set. In many cases, the original and masked data sets would have the
same size $N_0$, in which case, $N = 2N_0$ and $c = 1/2$. When the
original and masked data have the same distribution, the propensity
scores for all units should approximately equal $c$, so that $U_p$ is
near zero. At the other extreme, if $\hat{p}_i$ is nearly one for
units $i$ from the masked data and nearly 0 for units from the
original data, then the two data sets are completely distinguishable
and $U_p \sim 1/4$.


This measure is sensitive to the specification of the logistic
regression used to estimate the propensity scores.  For example, using
an intercept only in the regression results in $\hat{p}_i = c$ for all
$i$, regardless of the values in the masked data.  The advice from the
literature on propensity score estimation is
useful in the data utility context as well: include all variables,
with interactions and polynomial terms, considered important to make
similar in the original and masked data.

Note, that propensity score metric is not tied to the nature of the
masking.  This allows us to compute utility values on the same scale
for any masking strategy, which facilitates comparisons of the data
quality achieved by competing strategies applied on the same data
set.

Also note, that generic metrics of data utility may be blunt in that they do not necessarily distinguish among variables. For example, an \SDL\ procedure that produces very different distributions for a subset of substantively important predictors, but matches well on the subset of substantively unimportant predictors, could be rated as higher utility than a procedure that produces the opposite effects.  Similarly, minimizing the value of the generic utility metric may not lead to optimal \DBREL\ for certain conditional distributions.  

\subsection{Analysis specific metrics}
In this section we describe several analysis-specific or so called ``narrow" measures of data utility. There are many possible ways of defining such type of metrics,  they 
are linked to the types of analyses the user would like to do on the original data.
 Obviously, we cannot cover all of them, so below we present some examples 
 that can help to illustrate the idea of such metrics.



\subsection{Confidence Interval Overlap Utility Measures}\label{subsec.ci}

Data users and analysts  are frequently interested in fitting regression models. 
This process produces not only point estimates of the coefficients, but
confidence intervals as well. Thus, it is desirable for utility measures 
to indicate when the inferences, and
not just the point estimates, from regressions using the released
data are close to the corresponding ones using the original data.

Confidence intervals are main mechanism of inference in regression
models. Therefore, one measure of utility is the degree of overlap
between confidence intervals obtained from the same regressions
fit using the \DBREL\ and \DBORIG. The greater the overlap, the
higher the utility.

\subsubsection{Interval Overlap Metric}
Consider a fixed regression on the data, with specified response
and predictors. Let $(L_{\mathrm{rel},k}, U_{\mathrm{rel},k})$ be
the lower and upper limits of the 95\% confidence interval for the
regression coefficient $\beta_{k}$ obtained from \DBREL, and let
$(L_{\mathrm{orig},k}, U_{\mathrm{orig},k})$ be the corresponding
interval obtained from \DBORIG. Let $f_{\mathrm{rel},k}$ and
$f_{\mathrm{orig},k}$ be the estimated posterior distributions of
$\beta_k$ computed under \DBREL\ and \DBORIG, respectively.  For
example, in linear regression, $f_{\mathrm{orig},k}$ is the usual
$t$-distribution on $n-p$ degrees of freedom with mean
$\hat{\beta}_{\mathrm{orig},k}$ and variance the $k$th diagonal
element in
$\hat{\sigma}^2_{\mathrm{orig}}\left(X^{'}_{\mathrm{orig}}
X_{\mathrm{orig}}\right)^{-1}$, where
$\hat{\sigma}^2_{\mathrm{orig}}$ is the estimated residual
variance obtained from fitting the regression of
$Y_{\mathrm{orig}}$ on the associated $n \times p$ matrix of
predictors, $X_{\mathrm{orig}}$, which includes a vector of ones
for the intercept.

The probability overlap in the confidence intervals for
any $\beta_{k}$ \citep{kkors06} is defined to be equal to:
\begin{equation}
I_k = \frac{1}{2}
\left[\int_{L_{\mathrm{rel},k}}^{U_{\mathrm{rel},k}}
f_{\mathrm{orig},k}(t) dt +
\int_{L_{\mathrm{orig},k}}^{U_{\mathrm{orig},k}}
f_{\mathrm{rel},k}(t) dt \right]
\end{equation}
and the interval overlap measure, \IO, as
\begin{equation}
I = \sum_{i=1}^p I_k / p
\end{equation}
where $p$ is the dimension of the predictor variable matrix, including
the intercept.

By design, $0 \leq I_k \leq 0.95$ (as is the case for $I$), with
effectively no overlap corresponding to $I_k =0$ and perfect
overlap corresponding to $I_k = 0.95$.  Averaging the two
integrals in the definition of $I_k$ helps deal with cases where
$(L_{\mathrm{orig},k}, U_{\mathrm{orig},k}) \subseteq
(L_{\mathrm{rel},k}, U_{\mathrm{rel},k})$, or vice versa.  For an
illustrative example, consider the case where
$(L_{\mathrm{orig},k}, U_{\mathrm{orig},k}) = (8, 10)$, and for
two different proposed releases the $(L_{\mathrm{rel_1},k},
U_{\mathrm{rel_1},k})=(-12, 30)$ and $(L_{\mathrm{rel_2},k},
U_{\mathrm{rel_2},k})=(3, 15)$. From a utility perspective, the
second release is clearly preferable over the first release. The
\IO\ as defined favors the second release.  A criterion that just
equals $\int_{L_{\mathrm{rel},k}}^{U_{\mathrm{rel},k}}
f_{\mathrm{orig},k}(t) dt$ does not clearly distinguish the
releases, since this integral for both procedures is essentially
one. Similar examples can be constructed to show the inadequacy of
using $\int_{L_{\mathrm{orig},k}}^{U_{\mathrm{orig},k}}
f_{\mathrm{rel},k}(t) dt$ alone.

The \IO\ does not distinguish among intervals that have $I_k$
essentially equal to zero, some of which may be ``less worse''
than others. To adjust for this, the measure can be modified by
adding some distance-based penalty when $I$ is essentially zero,
or perhaps even when $I_k$ is essentially zero for some $k$, where
distance is defined as some function of the
$|\hat{\beta}_{\mathrm{rel},k} - \hat{\beta}_{\mathrm{orig},k}|$
or of $\min\left\{|L_{\mathrm{rel},k} - U_{\mathrm{orig},k}|,
|L_{\mathrm{orig},k} - U_{\mathrm{rel},k}|\right\}$.

An alternative measure is the overlap in the interval lengths. Let
$(L_{\mathrm{over},k}, U_{\mathrm{over},k})$ be the overlap in
these intervals, defined as $\left\{b: b \geq L_{\mathrm{orig},k},
b \geq L_{\mathrm{rel},k},  b \leq U_{\mathrm{orig},k},  b \leq
U_{\mathrm{rel},k}\right\}$. Then, the average relative overlap in
the confidence intervals for any $\beta_{k}$ equals:
\begin{equation}
J_k = \frac{1}{2} \left[\frac{U_{\mathrm{over},k} -
L_{\mathrm{over},k}}{U_{\mathrm{orig},k} - L_{\mathrm{orig},k}} +
\frac{U_{\mathrm{over},k} -
L_{\mathrm{over},k}}{U_{\mathrm{rel},k} -
L_{\mathrm{rel},k}}\right].
\end{equation}
The interval overlap measure then could be defined as $J = (1/p)
\sum_{i=1}^p J_k$.

\subsubsection{Ellipsoid Overlap Metric}\label{subsec.overlap}
The \IO\ measure considers each interval separately, effectively using all
the conditional distributions of the coefficients rather than
their joint distribution. Some analysts may be interested in
simultaneous intervals, which are defined by multidimensional
ellipsoids.  So, ellipsoid overlap measure \EO\ is constructed 
based on  posterior probabilities of regions defined by ellipsoids, that is, 
Bayesian perspective is used.  Generically, let $\hat{\beta}$ be the
maximum likelihood estimate of $\beta$, the $p \times 1$ vector of
true coefficients in the regression of $Y$ on $X$, and let
$\hat{\sigma}^2$ be the estimated residual variance for that
regression. Under the standard linear regression assumptions and
assuming standard non-informative prior distributions for $\beta$
and $\sigma^2$, the $(1-\alpha)100\%$ joint highest posterior
density ellipsoid for $\beta$ is defined by all the values of
$\beta$ such that
\begin{displaymath}\label{eq_1}
\frac{(\beta
-\hat{\beta})^T(X^{T}X)(\beta-\hat{\beta})}{p\hat{\sigma}^2} \leq
F(\alpha;p, n-p)
\end{displaymath}
where $F(\alpha;p, n-p)$ is the critical value from the $F$
distribution with $p$ and $n-p$ degrees of freedom.  The ellipsoid
from the \DBORIG, which we call $E_{\mathrm{orig}}$, is obtained
by setting $\hat{\beta} = \hat{\beta}_{\mathrm{orig}}$,
$\hat{\sigma}^2 = \hat{\sigma}^2_{\mathrm{orig}}$, and $X =
X_{\mathrm{orig}}$. The ellipsoid from the \DBREL, which we call
$E_{\mathrm{rel}}$, is obtained by setting $\hat{\beta} =
\hat{\beta}_{\mathrm{rel}}$, $\hat{\sigma}^2 =
\hat{\sigma}^2_{\mathrm{rel}}$, and $X = X_{\mathrm{rel}}$.

The utility measure \EO\ is the average of two posterior
probabilities: 1) the probability of $E_{\mathrm{orig}}$ computed
using the posterior distribution of $\beta$ based on \DBREL, and
2) the probability of $E_{\mathrm{rel}}$ computed using the
posterior distribution of $\beta$ based on \DBORIG. To determine
these probabilities,  Monte Carlo simulations can be used. For the first
probability, we draw values of $\beta$ from its posterior
conditional on \DBREL\ which is a $p$-variate t-distribution with
mean $\hat{\beta}_{\mathrm{rel}}$ and covariance matrix
$\hat{\Sigma}_{\mathrm{rel}} =
\hat{\sigma}^2_{\mathrm{rel}}(X_{\mathrm{rel}}^{t}X_{\mathrm{rel}})^{-1}$
with $n-p$ degrees of freedom. We then calculate the percentage of
these drawn $\beta$ that lie within $E_{\mathrm{orig}}$.  A
similar process is used to obtain the second probability by
drawing from the posterior of $\beta$ given \DBORIG\ and finding
the percentage of these that lie inside $E_{\mathrm{rel}}$. As
with \IO, the \EO\ can be extended to any parameters whose
distribution is well-approximated by a multivariate normal
distribution.

% Here: start removing from here or drastically reducing, we may just say that 
% such and such methods have generally such and such utility or compare in such a way with other methods,
% say noise is better than mixroaggregation and microaggregation is better than swapping - or simething like that.

\subsection{On Utility Properties of some families of \SDL\ methods}
\label{subsec.simulateddata}

There is a wide plethora of SDL methods and they differ significantly in terms of utility  and risk. Another source of variability of SDL methods in terms of utility (and risk as well) is due to the fact that SDL methods can be applied with a different degree of intensity, that is, data protector can chose  different values of parameters for these methods. 
For example, for Noise addition, smaller or larger variance of Noise can be chosen, which will affect the utility and risk of the masked data. For Swapping, the percentage of swapped records can be different, for Rankswappping  the  maximal allowed difference in the ranks of the swapped records  may vary (which is a parameter of Rankswapping). For Microaggregation, a minimal number of records per group should be set up by the data protector.  There is a wide variety of SDL methods and  infinitely many method-parameter combinations each leading to a masked data set with different utility and risk. Hence, we do not intend to provide a comprehensive utility comparisons here, but instead to present some examples of generic utility characterizations of different families of SDL methods . 

One of the largest families of SDL methods (in terms of different  implementations) is a  Microaggregation family. It can   be divided into Multivariate Microaggregation   and  Univariate Microaggregation. The later group includes Microaggregation with individual ranking which consists of microaggregating each variable individually and independently from other variables and Microaggregation using projections. The later one is usually accomplished by ranking  multivariate data by projecting them onto a single axis, using either the sum of $z$-scores or the first principal component, and then aggregating data into groups of size $k$, except possibly for one group of larger size (from $k+1$ to $2k -1$).
Of all these methods, Microaggregation with individual ranking is typically the least perturbative method \citep{kkors06}. Typically, masked values obtained using this method are close to the corresponding original ones, and thus analyses performed on the masked data often lead to a very similar results to those obtained on the original data. For example, \EO\ utility metric is often high (close to $1$)  for this method \citep{kkors06}. On the other hand, the risk of re-identification using record linkage approach is high for this method as well. 
In contrast to Microaggregation using Individual Ranking, projection-based Microaggregation methods and Multivariate Microaggregation may introduce significant perturbation to the data with utility ranging from average to low according to \IO\ and \EO\ . But the re-identification risk (estimated based on record-linkage experiments) is low as well \citep{}. 

One of the desirable features of Microaggregation methods is that they  inherently satisfy requirements of k-anonymity (if this criterion for Risk  is adopted by the data protector of course). Also microaggregation methods preserve means of the original data, they preserve positivity/non-negativity  constraints in the data (if the original values are positive, so are the masked values)  which in some instances of data release  is a desirable feature. Microaggregation methods, on the other hand have a shrinking effect on the original data by reducing  the variance of the original data.
 
 $ $ 

Another large family of methods is Noise infusion, which can be implemented as additive or multiplicative noise for numerical variables.
Basic implementations of noise addition preserve  mean and the correlation structure of the original data which is an important for some statistical analyses, such as linear regression. Using data transformation Noise addition can be implemented in such a way so that not only correlation matrix  but covariance matrix is preserved as well.
 In it's basic implementation noise methods do not preserve positivity or non-negativity constraints, that is, if original variables are non-negative then masked variables are also non- negative. Multiplicative noise on the other hand can preserve positivity constraints.
 % Here: About metric-related performance of noise.
 
Standard implementations of additive and multiplicative noise does not satisfies the requirements of k-anonimity.
Noise generated from Normal distribution with specially calibrated variance, as well as noise generated from Laplace distribution with special parameters  satisfies requirements of  differential privacy.
Record linkage experiments showed that additive noise has low re-identification risk.



{\bf To be continued... A few more SDL families will be described/compared in terms of Utility}
 
\section{Donna's stuff}
ACCESS TIER: PROTECTED
• Restricted-use data behind firewall with output controlled for disclosures;
• Automated output from SDL software with use restrictions (e.g. web-based query system)
• Licensing program (user controlled infrastructure)

Access to this data is not automatic but requires one or more additional steps such as, but not limited to, a data use agreement, license, system or website account, and automated SDL tools with built-in limitations on allowable output. The number of additional requirements for access is intended to be more than the public access, but less burdensome than the restricted tier. The lowered burden should improve access, timeliness, and other aspects of data quality in a meaningful way to the data customer.

Web-Based Query Systems 
Query systems allow users to design queries to generate customized tabulations (WP 22). Also have predefined queries.

*Data stored in query systems can be protected and restricted.

Pros 
•	Run queries on data more detailed than PUFs (varies by agency—could be restricted data or perturbed data from public use files) 
o	Increases utility, improves data quality
•	Permits a wider range of analyses than does releasing only data summaries and it provides results based on actual rather than simulated microdata (Gomatam, 2005, 164)
•	Developer can build in SDL (see Gomatam, 2005, 167)

Cons 
•	No direct access to the microdata (may vary by agency)
o	Decrease utility
•	May not prevent disclosures (table splicing; can do this in CDC Wonder) 
•	Built in disclosure technique can reduce utility and quality (Gomatam, 2005, 167-168)
o	Top coding, swapping, adding noise (WP 22)
o	Prohibit key identifiers (age, race, sex) as outcome variables but permit as predictors (Gomatam, 2005, 167)
o	Disallow any transformations (Gomatam 2005, 173)
•	Functionality is limited to what the developer allows to be run in the query system
•	Expensive to implement and maintain (Haggard, 2006, 189)

New queries vs. pre-computed queries 

Examples of online query systems: 
•	NASS (Karr)
•	Australian Bureau of Statistics, TableBuilder (Chipperfield, 2019)
•	CDC’s Wonder
•	BLS 
 

ACCESS TIER: RESTRICTED ACCESS
• Licensing program (user controlled infrastructure)
• Virtual Data Enclave
• Physical Data Enclave

This is the default tier for containing data collected under a pledge of confidentiality. Critical to its modernization is to include more access options using newer technology such as commercial cloud computing and remote access/virtual enclave. This should also include viable options not constrained by the current system of Federal Statistical Research Data Centers (FSRDCs).

Virtual and Physical Data Enclaves 
There are two types of data enclaves: 1) physical data enclaves; and 2) virtual data enclaves. Both types of enclaves allow researchers to access restricted use data under highly restricted conditions which reduces disclosure risks. Some data enclaves allow researcher to access the full data sets while others require researchers to prepare data dictionaries and limit access to only the variables the researchers need to complete their analysis. 

The primary difference between the two types of data enclaves is the process by which the data are accessed. In physical data enclaves researchers must physically sit in a controlled environment at the data owner’s office or site where the data are stored. Virtual data enclaves allow researchers to access the restricted use data remotely over secure electronic lines via their personal computers while they sit in their own offices or homes. The output generated is returned to the researchers.

Both types of data enclaves increase the usability of the data. Researchers are permitted to access data not publicly available under controlled conditions. For example, [list types of data that cannot be publicly accessed: genetics data, geocoded data, detailed geography, exact dates, detailed race, income]. 

Conversely, both types of data enclaves can also decrease the utility of the data. The process for requesting access to restricted use data can be arduous and thus reduce the number of researchers who can access the data. Researchers must submit research proposals containing detailed information about the research project, the hypotheses to be tested, the data set and variables to be used in the analysis,
the empirical methods to be used, and the specific data outputs that will result from the project thus limiting exploratory analyses. Research proposals are reviewed and approved by a review committee which can take several weeks or months to complete. Additionally, users must agree to terms and conditions governing the access and use of the confidential data as well as sign nondisclosure affidavits. Some researchers are required to complete background investigations, Special Sworn Status, be citizens of the U.S.,  …etc.  Furthermore, there are costs associated with accessing restricted use data via data enclaves [include examples]. Costs reduce the utility of the data because some researchers may not have funding to complete research. Most students completing graduate or doctoral level research, living on fixed incomes, may not be able to afford to access data in enclaves. 

Both types of data enclaves allow researchers to improve the accuracy and precision of their estimates. Data available in enclaves are not subject to the statistical disclosure limitation methods that public use files are subject to prior to release. For example, detailed race/ethnicity and geography measures are typically not available on public use files due to disclosure concerns. These types of measures are available to researchers in data enclaves thus increasing the accuracy and prevision of estimates.  [Might include examples of research completed in RDCs that could not be completed using PUFs].

Data quality can also be decreased when accessing restricted use data in data enclaves. Extreme values or values representing an individual are generally removed from analysis (e.g. minima, maxima, medians). These values might be useful to researchers doing sensitivity analysis. {need to expand this section}

Licensing Program (user controlled infrastructure)

Licensing agreements permit a researcher to use restricted data offsite, but under highly restricted
conditions, as spelled out in a legally binding agreement [text from Restricted Access Procedures]. Arrangements that place restrictions on who has access, at what locations, and for what purposes access is allowed normally require written agreements between agency and users. These agreements usually subject the user to fines, being denied access in the future and/or other penalties for improper disclosure of individual information and other violations of the agreed conditions of use. Users may be subject to external audits conducted by the agency to assure terms of the agreement are being followed. Users in violation may be required to pay fines or be subject to other legal penalties [text from WP 22].

Licensing agreements require:
- a demonstrated need for sensitive data;
- authorization for all users at the requesting institution;
- signature by a senior level official and key staff;
- a data security plan;
- agreement by researchers not to identify individual research subjects or to link data received with other microdata files; and
- review of all statistical output before publication.
[text from Restricted Access Procedures].

The license is for is a specified period of time and data files must be returned or destroyed. Some licensors require fees and/or approval by an institutional review board. Additional information on this method is provided here: https://nces.ed.gov/FCSM/pdf/CDAC\_RAP.pdf

Pros for Licensing Program
•	…
•	…

Cons for Licensing Program
•	…
•	…






 

Future Placeholder for Highly Restricted Access Tier
(leave blank)



\section{Ellen's stuff - Public Use Microdata}

**Note - Ellen will edit and add to this Thursday and add citations, but I wanted to add something for now.

Many statistical agencies provide microdata files directly to the public, available online without requiring users to register or provide information about what they will do with it. 

One advantage of making public use files available is that the data can reach a much wider audience and be used much more broadly. They are generally made free for anyone who would like access, which makes them particularly appealing for students and researchers alike.

Depending on the data, they can also be extremely easy for people to use and understand. While some may require extensive codebooks and statistical knowledge to manipulate, some government agencies are moving toward online query systems that allow anyone to determine information with little effort. For example, Baltimore city has a public database of all 911 calls that can be downloaded if the user would like to. However, they also have a query system on their website that allows users to look at combinations of variables, such as locations, date and times, and types of emergencies. With a few mouse clicks, any user can create a visualization of the data that can include multiple variables. 

Providing more people easier access to microdata can allow for more queries to be run, more data to be combined, and more research to be conducted that can change public policy. Most government agencies conduct marketing research and implement marketing techniques with the goal of having more people use their data, whether tabular or microdata. By allowing access to microdata, however, there is more information that can be gathered from the data, it can be used in additional ways, and has the potential to influence more research, more policies, and more people.

In addition, wide access to public use data can often allow for those datasets to be combined with other confidential datasets that likely wouldn’t be possible if the data were only available in a restricted use setting. Most restricted use access options, such as FSRDCs or virtual data enclaves, prevent users from removing the data in any means from their location. Public use files can usually be uploaded into these settings, but because it is not possible to pull the restricted use files from their location, it is often impossible to combine two restricted use files and draw conclusions from the resulting datasets. Public use files allow data users to combine the data with confidential datasets, which can result in conclusions that would not be possible to draw without public access to the data. 

Of course, the practice of making access to data easier, especially unperturbed microdata, increases the risk of it being used for malfeasance. Even data that are believed to be anonymized can often be used to determine information about specific respondents. Examples of misused public data are numerous, from determining preferences of Netflix users  to identifying the governor of Massachusetts in medical records by pairing it with public voter identification information . The Panel of Data Access for Research Purposes proposed two recommendations to limit de-identification attacks on government data. First, all users should be notified when accessing government data that it was collected with the legal requirement to be used for only statistical purposes and users should be required to acknowledge that they have read the disclaimer before the data can be viewed. Second, government institutions should impose ``meaningful’’ penalties for users who use the data for something other than statistical purposes, such as de-identification attempts. While that recommendation was made by the panel in 2005, many government agencies have not taken these steps and still allow access to their microdata without reading a disclaimer and have no real penalties in place for misuse of the data.

While the United States has decentralized statistical agencies which largely create their own rules on data access, other countries often have tiers of access that are standard for all data programs. Stats Canada, for example, has certain datasets that are available publicly and certain datasets that can only be accessed through data research centers or remote access that is granted for researchers. However, there are also certain datasets for which Stats Canada allows researchers to access tabular data by submitting statistical programs without seeing the underlying microdata, for which tabular data is returned. This intermediate data access level can allow the program to apply disclosure methods to the tabular data but allow researchers to use the microdata, which can add some protections for the respondents.

The United Kingdom Data Center has three levels of access, as well: open, safeguarded, and controlled data. Open data is believed to be completely free of identifying data and can be used by anyone without permission or registration. Safeguarded data is believed not to have identifying information, but there could be concerns about the possibility of identifying respondents by linking the information with other dataset. To gain access to these data, users need to register and agree to certain conditions, such as not using the data to identify individuals or disseminate any identifying information. Finally, controlled data is believed to contain information that could be disclosive and is protected through a number of means, such as requiring users to register and be approved, as well as complete training to access the data.

For both Stats Canada and the UK Data Center, each dataset is evaluated for risk and the microdata are assigned a level of protection. In some cases with risky data, the data are altered in some way, such as by removing variables, to decrease the risk. This can allow different versions of the same dataset to be available at different access levels, so researcher with training and appropriate credentials can gain access to the full data while other users who may not have the necessary credentials or may not need the full dataset can use the modified data. This gives the advantage of wide access to the data while protecting the most vulnerable elements of information from the respondents.

Disclosure Protection in Public Use Files

When datasets are release for public use, they generally have some modifications made to reduce risk to respondents. That does not mean that all risk is eliminated, but most government agencies at least attempt to limit the likelihood of re-identification. At the very least, this means removing respondent identifying information, such as names, addresses, and dates of birth from the data. In many cases, this is not removed from the file entirely, but changed into some sort of aggregate variable like county or state rather than address, or year of birth rather than exact date. While this helps to protect the respondents, it can also reduce the utility of the file. If state is the only information provided regarding the location of the respondent, it prevents studies of rural versus urban respondents.

In addition, many variables in public use microdata are rounded, top-coded, or organized into certain groups. Again, these methods may reduce some amount of utility, but can protect respondents. With the availability of information available that can be merged with public use data, exact information about variables such as dollar amounts, especially for outliers, can pose a risk. However, these methods can also skew; if wages are top-coded so all earnings above a certain level are written as something lower, the averages will no longer accurately reflect the overall population.

More surveys are beginning to use some form of data perturbation to create the microdata file that is released to the public. 
The American Community Survey (ACS) at the Census Bureau, which collects information from 2.3 million housing units per year, will move to creating synthetic data and release that as their public use microdata file, rather than the actual collected data. They have begun testing methods for creating the synthetic data, but so far, only some of the properties in the original data are reflected in the synthetic data. At geographic areas lower than the state, there are even fewer properties of the original data reflected in the synthetic microdata file.

Some surveys, such as the National Longitudinal Survey of Youth at the Bureau of Labor Statistics, release public use data files with most variables include, but lacking almost all geographic information. While that helps to provide protection for respondents to the surveys, it makes it impossible to make determinations about variations across locations.




\section{Data Access - from LARS}
\input{05_02_state_of_physical_security}

%\bibliographystyle{apalike}
\bibliography{dg2-new,from_zotero}
\bibliographystyle{econ}

\section*{Glossary}
To use the glossary, use 
\begin{verbatim}
    \gls{term}  for lower-case version
    \Gls{term} for upper-case version
    \glspl{term}  for lower-case plural
    \Glspl{term} for upper-case plural version
\end{verbatim}
Example
\begin{quote}
    The use of \gls{differential_privacy} is expanding.
\end{quote}

\glsaddall
\printglossary


\end{document}
